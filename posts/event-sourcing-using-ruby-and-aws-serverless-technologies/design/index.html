<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><title>Event Sourcing with Ruby and AWS Serverless Technologies - Part One: Design - apiercey.github.io</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=https://apiercey.github.io/favicon.png><meta name=google-site-verification content="NumVaC4w3L9xrwd7wA91DomOegJLYQ-jIhXhWmK7Yd0"><link rel=stylesheet href=/css/style.min.b5158346cd0be7c685bb4907526f1b697a94560681b975b89e813e6095001706.css><meta name=description content="Design of an event sourcing system and how Change Data Capture is achieved. Examines how this is accomplished using DynamoDB, Kinesis, Lambda, S3, SQS, and Terraform."><meta name=keywords content="event sourcing,ruby,aws,serverless,change data capture,cdc,architecture,terraform"><meta property="og:title" content="Event Sourcing with Ruby and AWS Serverless Technologies - Part One: Design"><meta property="og:type" content="website"><meta property="og:url" content="https://apiercey.github.io/posts/event-sourcing-using-ruby-and-aws-serverless-technologies/design/"><meta property="og:image" content="https://apiercey.github.io/images/aws-eventsourcing/header.jpg"><meta property="og:description" content="Design of an event sourcing system and how Change Data Capture is achieved. Examines how this is accomplished using DynamoDB, Kinesis, Lambda, S3, SQS, and Terraform."><meta name=twitter:card content="summary"><meta name=twitter:site content="@programincolour"><meta name=twitter:creator content="@programincolour"><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;700&display=swap" rel=stylesheet></head><body class='page page-blog-single'><div id=menu-main-mobile class=menu-main-mobile><ul class=menu><li class=menu-item-about><a href=https://apiercey.github.io/pages/about/>About</a></li><li class=menu-item-blog><a href=https://apiercey.github.io/posts/>Blog</a></li><li class=menu-item-projects><a href=https://apiercey.github.io/projects/>Projects</a></li><li class=menu-item-articles><a href=https://apiercey.github.io/articles/>Articles</a></li></ul></div><div id=wrapper class=wrapper><div class=header><a class=header-logo href=/>apiercey.github.io</a><div class=menu-main><ul><li class=menu-item-about><a href=/pages/about/><span>About</span></a></li><li class="menu-item-blog active"><a href=/posts/><span>Blog</span></a></li><li class=menu-item-projects><a href=/projects/><span>Projects</span></a></li><li class=menu-item-articles><a href=/articles/><span>Articles</span></a></li></ul></div><div id=toggle-menu-main-mobile class=hamburger-trigger><button class=hamburger>Menu</button></div></div><div class=blog><div class=intro><h1>Event Sourcing with Ruby and AWS Serverless Technologies - Part One: Design<span class=dot>.</span></h1><img src=/images/aws-eventsourcing/header.jpg></div><div class=content><aside><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-idea-of-a-shopping-cart>The Idea of a Shopping Cart</a></li><li><a href=#how-event-sourcing-works>How Event Sourcing Works</a><ul><li><a href=#what-problem-does-it-solve>What Problem Does It Solve?</a></li><li><a href=#how-publishing-works>How Publishing Works</a></li><li><a href=#example-change-data-capture-in-postgresql>Example: Change Data Capture in PostgreSQL</a></li><li><a href=#example-change-data-capture-in-mongodb>Example: Change Data Capture in MongoDB</a></li></ul></li><li><a href=#aws-technologies-and-their-roles>AWS Technologies and Their Roles</a><ul><li><a href=#lambda>Lambda</a></li><li><a href=#dynamodb-and-dynamodb-streams>DynamoDB and DynamoDB Streams</a></li><li><a href=#kinesis-streams>Kinesis Streams</a></li><li><a href=#s3-bucket>S3 Bucket</a></li><li><a href=#sqs-queues>SQS Queues</a></li><li><a href=#cloudwatch-logs>CloudWatch Logs</a></li></ul></li><li><a href=#the-architecture>The Architecture</a></li><li><a href=#infrastructure-as-code-and-terraform>Infrastructure as Code and Terraform</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></aside><p>This is the first part in an on-going blog series about <a href=/posts/event-sourcing-using-ruby-and-aws-serverless-technologies/introduction>building an event sourcing system in Ruby using AWS Serverless technologies</a>.</p><h2 id=the-idea-of-a-shopping-cart>The Idea of a Shopping Cart</h2><p>The idea of a Shopping Cart gives us a nice foundation for building an event sourced system. It is a familiar concept and introduces a type of temporal model, which helps solves problems with our data changing <em>over time</em>.</p><p>For example, when shoppers come to your site to purchase merchandise, you may want to know when they Add an Item to an Open Cart (a change) so related items can be suggested to them. Or perhaps, if a cart is still Open after two weeks, an email is sent to them reminding them of the items they have in their Open Cart.</p><p>For this project, we will build a small Shopping Cart application to explore event sourcing. It will have three primary requirements: opening a new cart, adding items to an existing cart, and closing an existing cart.</p><h2 id=how-event-sourcing-works>How Event Sourcing Works</h2><p>Event sourcing is a persistence pattern that, instead of storing <a href=https://martinfowler.com/bliki/DDD_Aggregate.html>aggregates</a> as whole objects in a database and making updates against them, stores them as a series of events. The first event is the event that <em>starts</em> the aggregate and every subsequent event is a change the aggregate has gone through.</p><p>On the other hand, to retrieve an aggregate from the database, it does not retrieve a whole object but rather all related events to the aggregate. Once these events have been retrieved, the aggregate is rehydrated (rebuilt) by iterating over each event, building up to the most recent state.</p><img src=/images/aws-eventsourcing/shopping-cart-lifecycle.jpg width=500px style=display:block;align-self:center;margin-left:auto;margin-right:auto><p>This is very similar to reducing an array of structures into a final structure. Take this array of hashes for example:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ruby data-lang=ruby><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-1><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-1>1</a></span><span>aggregate <span style=color:#ff6ac1>=</span> {<span style=color:#ff5c57>name</span>: <span style=color:#5af78e>&#34;CatMeme&#34;</span>}
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-2><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-2>2</a></span><span><span style=color:#ff6ac1>=&gt;</span> {<span style=color:#5af78e>:name</span><span style=color:#ff6ac1>=&gt;</span><span style=color:#5af78e>&#34;CatMeme&#34;</span>}
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-3><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-3>3</a></span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-4><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-4>4</a></span><span>events <span style=color:#ff6ac1>=</span> <span style=color:#ff6ac1>[</span>{<span style=color:#5af78e>status</span>: <span style=color:#5af78e>&#34;Uploaded&#34;</span>}, {<span style=color:#5af78e>featured</span>: <span style=color:#ff6ac1>true</span>}<span style=color:#ff6ac1>]</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-5><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-5>5</a></span><span><span style=color:#ff6ac1>=&gt;</span> <span style=color:#ff6ac1>[</span>{<span style=color:#5af78e>:status</span><span style=color:#ff6ac1>=&gt;</span><span style=color:#5af78e>&#34;Uploaded&#34;</span>}, {<span style=color:#5af78e>:featured</span><span style=color:#ff6ac1>=&gt;</span><span style=color:#ff6ac1>true</span>}<span style=color:#ff6ac1>]</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-6><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-6>6</a></span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-7><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-7>7</a></span><span>events<span style=color:#ff6ac1>.</span>reduce(aggregate, <span style=color:#5af78e>:merge</span>)
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-0-8><a style=outline:none;text-decoration:none;color:inherit href=#hl-0-8>8</a></span><span><span style=color:#ff6ac1>=&gt;</span> {<span style=color:#5af78e>:name</span><span style=color:#ff6ac1>=&gt;</span><span style=color:#5af78e>&#34;CatMeme&#34;</span>, <span style=color:#5af78e>:status</span><span style=color:#ff6ac1>=&gt;</span><span style=color:#5af78e>&#34;Uploaded&#34;</span>, <span style=color:#5af78e>:featured</span><span style=color:#ff6ac1>=&gt;</span><span style=color:#ff6ac1>true</span>}
</span></span></code></pre></div><p>The final state is single structure.</p><h3 id=what-problem-does-it-solve>What Problem Does It Solve?</h3><p>Event Sourcing solves the difficult problem of when a message needs to be published alongside a data change, informing interested parties about that change. This problem often arises in both distributed systems and eventually consistent systems. Consider the following:</p><ul><li>What should happen to your changed data if publishing the message to the message broker should fail?</li><li>What should happen to a published message if change fails during transaction commit against the database?</li></ul><p>The intention is to provide a solution that does not introduce a <a href=https://en.wikipedia.org/wiki/Two-phase_commit_protocol>Two-Phase Commit</a>. Even if you could introduce Two-Phase Commits, maintaining a Transaction Co-Ordinator comes with painful pitfalls and &lsquo;<em>gotchas!</em>&rsquo;. For example, what happens when one system in a distributed transaction is exhausted? The entire system could come to a standstill. Yuck!</p><h3 id=how-publishing-works>How Publishing Works</h3><p>As odd as it may sound, we are actually able to leverage the underlying database technology to help with publishing messages.</p><p>Many clever database designers have solved the challenges that come with building a database by introducing a sort of <em>log of data changes</em> which is used to guarantee data consistency and decrease latency.</p><p>Some examples in popular databases are:</p><ul><li>Write Ahead Log (WAL) in PostgreSQL</li><li>Redo Log in InnoDB, used by MariaDB, MySQL</li><li>Journaling in MongoDB</li></ul><p>Even more clever, the designers provided programmers with the means to <em>hook</em> into these logs for our own needs! These are often called <em>streams</em> and even come first-class in some databases such as <a href=https://www.mongodb.com/basics/change-streams>MongoDB Streams</a> or <a href=https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html>DynamoDB Streams</a>. However, in some cases like PostgreSQL, some <a href=https://datacater.io/blog/2021-09-02/postgresql-cdc-complete-guide.html>additional tricks</a> are needed to publish data changes outside of the database.</p><p>This process is known as <a href=https://en.wikipedia.org/wiki/Change_data_capture><em>Change Data Capture</em></a> and often happens in three steps: <em>Capture, Transform, and Publish</em>.</p><h3 id=example-change-data-capture-in-postgresql>Example: Change Data Capture in PostgreSQL</h3><p>Change Data Capture in PostgreSQL is possible by leveraging <a href=https://www.w3resource.com/PostgreSQL/postgresql-triggers.php>SQL Triggers</a> so that whenever data is changed, we have Postgres execute a function for us. The role of this function is to, well, <em>capture</em> changes, transform them into a universal format (e.g. JSON), and push them to consumers who are interested in these changes.</p><p><img src=/images/aws-eventsourcing/postgres-cdc.jpg alt="CDC with PostgreSQL"></p><p>In an Event Sourcing setup, there is usually a single consumer which is the event stream. The trigger will capture newly added events to an aggregate and publish them to the stream.</p><h3 id=example-change-data-capture-in-mongodb>Example: Change Data Capture in MongoDB</h3><p>Many database designers are embracing the Change Data Capture concept and building this as a first-class feature in their databases. MongoDB is an excellent example of this.</p><p>Change Data Capture in MongoDB is facilitated by using <a href=https://www.mongodb.com/docs/manual/changeStreams/>Change Streams</a>. Internally, changes are captured by leveraging an already built-in feature called <em>replication</em>, which replicates data from the Primary node to Secondary nodes.</p><p>When a change is being replicated, it&rsquo;s &ldquo;pushed&rdquo; to a Change Stream. From there, consumers can consume these changes.</p><p><img src=/images/aws-eventsourcing/mongodb-cdc.jpg alt="CDC with MongoDB"></p><p>Like PostgreSQL, in an Event Sourcing setup, there is usually a single consumer, being the event stream. In the example above, the data still needs to be transformed from the format of how MongoDB publishes changes, to the format we expect from our events.</p><h2 id=aws-technologies-and-their-roles>AWS Technologies and Their Roles</h2><p>Our goal is to use only <a href=https://en.wikipedia.org/wiki/Serverless_computing>serverless</a> technologies as this will help us scale on demand and be highly available. In addition, I have found that Event Sourcing is largely an <em>infrastructure</em> heavy pattern and comes with considerable maintenance overhead. Serverless relieves Application and DeveOps engineers of this responsibility.</p><h3 id=lambda>Lambda</h3><p><a href=https://aws.amazon.com/lambda/>Lambda</a> will be our computing power. It will be is used for:</p><ol><li>Handle our incoming client requests whether they are coming from an API, background worker, or another piece of the infrastructure.</li><li>Provide event handlers for events published on our Event Stream.</li><li>Transform captured events from DynamoDB and publish them to Kinesis. More on this below.</li></ol><h3 id=dynamodb-and-dynamodb-streams>DynamoDB and DynamoDB Streams</h3><p><a href=https://aws.amazon.com/dynamodb/>DynamoDB</a> will be our database. It will provide tables for our aggregates to store events and streams to publish changes.</p><p>DynamoDB is the ideal choice because it is a key-value store that allows us to store unstructured events and is highly scalable to unexpected peaks of traffic.</p><h3 id=kinesis-streams>Kinesis Streams</h3><p>A <a href=https://aws.amazon.com/kinesis/data-streams/>Kinesis Stream</a> will act as our event stream. Events captured from DynamoDB tables will be published here, becoming available to our event handlers.</p><h3 id=s3-bucket>S3 Bucket</h3><p>An <a href=https://aws.amazon.com/s3/>S3 Bucket</a> will be used for long term storage of our events. One of the primary characteristics of event sourcing systems is their ability to query and evaluate historical events.</p><p>Kinesis only allows data retention of up to one year, so S3 makes a great tool for long-term use. Some non-AWS streaming technologies, such as <a href=https://kafka.apache.org/>Kafka</a>, address this by allowing <a href=https://stackoverflow.com/questions/39735036/make-kafka-topic-log-retention-permanent>log retention to be forever</a>.</p><p>That said, we&rsquo;ll also need to leverage Lambda to replay events and SQS to queue replayed events. More on this below.</p><h3 id=sqs-queues>SQS Queues</h3><p><a href=https://aws.amazon.com/sqs/features/>SQS</a> comes in two flavours: standard queues and First-In First-Out (FIFO) queues.</p><p>The standard queue allows consumers to retrieve messages without any guarantee of ordering. This is the most common setup amongst applications, especially when dealing with high-volume. However, when your application needs to guarantee <em>ordering</em>, this is where FIFO queues play a valuable part.</p><p>We&rsquo;ll be using FIFO to guarantee event order when event handler consume our replayed events.</p><h3 id=cloudwatch-logs>CloudWatch Logs</h3><p>While not part of our solution, our Lambdas will log to <a href=https://aws.amazon.com/cloudwatch/>CloudWatch</a>, so we can inspect any errors.</p><h2 id=the-architecture>The Architecture</h2><p><img src=/images/aws-eventsourcing/architecture.jpg alt="Architecture in AWS"></p><p>Requests will flow into our Lambdas which hosts our ShoppingCart business logic. When this happens, our application will rehydrate a Shopping Cart aggregate and execute a single business function against it.</p><p>Rehydration happens by retrieving events from a <em>DynamoDB table</em> and not from the Kinesis stream. In this regard, you can think of Kinesis stream as the &ldquo;all-events-stream&rdquo; and each DynamoDB as a stream for each &ldquo;aggregate&rdquo;. New events are persisted alongside old ones inside the table.</p><p>Change Data Capture is achieved using DynamoDB Streams. All events are <em>Captured</em> to an internal stream and handled by a Lambda to <em>Transform</em> them into JSON and <em>Publish</em> them to our Kinesis &ldquo;all-event-stream&rdquo;. Next, a &ldquo;storage&rdquo; Lambda will be used to store our published events in an S3 Bucket for long term storage and replayability.</p><p>At this point, Event Handlers have the opportunity to consume an event by subscribing to the Kinesis &ldquo;all-event-stream&rdquo;.</p><img src=/images/aws-eventsourcing/serverless-cdc.jpg width=800px style=display:block;align-self:center;margin-left:auto;margin-right:auto><p>Lastly, when it suits our needs, we can replay all historical events to any Event Handler required. A replay Lambda function will read all events stored in S3 and publish them to the Event Handler SQS queue.</p><p>This means our Event Handlers require <em>two</em> subscriptions: Kinesis Stream for new events and an SQS queue for replayed events.</p><img src=/images/aws-eventsourcing/serverless-replay.jpg width=400px style=display:block;align-self:center;margin-left:auto;margin-right:auto><h2 id=infrastructure-as-code-and-terraform>Infrastructure as Code and Terraform</h2><p>We&rsquo;re at the end of our first step in this series. However, it&rsquo;s critical to discuss <em>Infrastructure as Code</em> as it will sit at the core of our implementation.</p><p>Building infrastructure by hand is a very meticulous task and often error prone due to Layer 8 mistakes. Even more so, if the same infrastructure needs to be built within multiple environments, it can become seriously time inefficient to do this by hand!</p><p>This is where Infrastructure as Code (IaC) comes into play. as Code (IaC) comes into play. With IaC, infrastructure configuration and deployment can be automated and repeatable. Additionally, it makes managing environments easier and gives you a single source of truth.</p><p>Some examples of IaC are:</p><ul><li><a href=https://docs.chef.io/>Chef</a>, a Ruby DSL</li><li><a href=https://www.ansible.com/>Ansible</a>, a Python DSL</li><li><a href=https://aws.amazon.com/cloudformation/>CloudFormation</a>, AWS specific, written in JSON or YAML</li><li><a href=https://www.terraform.io/>Terraform</a>, implemented in GoLang.</li></ul><p>Be warned - not all IaC technologies are created equal! For this project we will use Terraform. In my experience, it does a fantastic job at achieving iachieving it staying out of the way of engineers. Additionally, it works across many Cloud providers and is a highly reusable skill.</p><p>An example of how powerful Terraform can be, let&rsquo;s look at an example. It&rsquo;s possible to boot up a PostgreSQL database in any environment with two simple steps:</p><p>First, add the following declaration to a file named <code>database.tf</code> (the file name does not particularly matter):</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-terraform data-lang=terraform><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-1><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-1>1</a></span><span><span style=color:#ff6ac1>resource</span> <span style=color:#5af78e>&#34;aws_db_instance&#34;</span> <span style=color:#5af78e>&#34;my-psql-db&#34;</span> {
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-2><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-2>2</a></span><span>  <span style=color:#57c7ff>allocated_storage</span>    = <span style=color:#ff9f43>10</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-3><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-3>3</a></span><span>  <span style=color:#57c7ff>db_name</span>              = <span style=color:#5af78e>&#34;my_postgres_db&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-4><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-4>4</a></span><span>  <span style=color:#57c7ff>engine</span>               = <span style=color:#5af78e>&#34;postgresql&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-5><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-5>5</a></span><span>  <span style=color:#57c7ff>instance_class</span>       = <span style=color:#5af78e>&#34;db.t3.micro&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-6><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-6>6</a></span><span>  <span style=color:#57c7ff>username</span>             = <span style=color:#5af78e>&#34;super_secure_username&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-7><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-7>7</a></span><span>  <span style=color:#57c7ff>password</span>             = <span style=color:#5af78e>&#34;super_secure_password&#34;</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-1-8><a style=outline:none;text-decoration:none;color:inherit href=#hl-1-8>8</a></span><span>}
</span></span></code></pre></div><p>Second, execute the apply command in the directory that hosts your terraform code:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f" id=hl-2-1><a style=outline:none;text-decoration:none;color:inherit href=#hl-2-1>1</a></span><span>$ terraform apply
</span></span></code></pre></div><p>Viola! You now have a Postgres database running in our AWS account. How wicked is that?</p><p><em>NOTE: In case you do try this, you can execute <code>terraform destroy</code> to remove the database and save yourself some money ;)</em></p><h2 id=conclusion>Conclusion</h2><p>We&rsquo;ve covered the design portion of our Event Sourcing application. We touched on the architecture and the technologies that make up its parts.</p><p>Stay tuned for the next step: <em>Ruby and Aggregates</em>, where we start implementing our design.</p></div><br><div class=comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://apiercey.github.io/posts/event-sourcing-using-ruby-and-aws-serverless-technologies/design/",this.page.identifier="gvjnq4v8bactn"};(function(){var e=document,t=e.createElement("script");t.src="https://apiercey.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div><div class=footer><div class=footer-social><span class="social-icon social-icon-github"><a href=https://github.com/apiercey title=github target=_blank rel=noopener><img src=/images/social/github.svg width=24 height=24 alt=github></a></span>
<span class="social-icon social-icon-linkedin"><a href=https://www.linkedin.com/in/alexander-butt-piercey title=linkedin target=_blank rel=noopener><img src=/images/social/linkedin.svg width=24 height=24 alt=linkedin></a></span>
<span class="social-icon social-icon-twitter"><a href=https://twitter.com/programincolour title=twitter target=_blank rel=noopener><img src=/images/social/twitter.svg width=24 height=24 alt=twitter></a></span></div></div></div><script type=text/javascript src=/js/bundle.min.28d149c65e8b43f935b5da6797544f9e363170785f466641db0d007cee894169.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CX84WRHVTJ"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-CX84WRHVTJ")</script></body></html>